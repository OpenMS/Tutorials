%!TEX root = handout.tex

\newpage
\section{Metabolomics}

\subsection{Introduction}
Quantitation and identification of chemical compounds are basic tasks in metabolomic studies. In this tutorial session we construct a UPLC-MS based, label-free quantitation and identification workflow. Following quantitation and identification we then perform statistical downstream analysis to detect quantitation values that differ significantly between two conditions. This approach can, for example, be used to detect biomarkers. Here, we use two spike-in conditions of a dilution series (0.5 mg/l and 10.0 mg/l, male blood background, measured in triplicates) comprising seven isotopically labeled compounds. The goal of this tutorial is to detect and quantify these differential spike-in compounds against the complex background.

%\begin{figure}[htbp]
%  \centering
%  \includegraphics[width=0.85\textwidth]{graphics/metabo/metabo_workflow}
%  \caption{Complete label-free metabolomics quantification workflow}
%  \label{fig:consensusid}
%\end{figure}

\subsection{Quantifying metabolites across several experiments}

For the metabolite quantification we choose an approach similar to the one used for peptides, but this time based on the OpenMS \KNIMENODE{FeatureFinderMetabo} method. This feature finder again collects peak picked data into individual mass traces. The reason why we need a different feature finder for metabolites lies in the step after trace detection: the aggregation of isotopic traces belonging to the same compound ion into the same feature. Compared to peptides with their averagine model, small molecules have very different isotopic trace distributions. To group small molecule mass traces correctly, an aggregation model tailored to small molecules is thus needed.

\begin{itemize}
\item
Create a new workflow called for instance "Metabolomics".
\item
Add a \KNIMENODE{Input Files} node and configure it with all mzML files from \directory{Example\_Data / Metabolomics / datasets}.
\item
Add a \KNIMENODE{ZipLoopStart} node and connect the \KNIMENODE{Input Files} node to the first port of the \KNIMENODE{ZipLoopStart} node.
\item
Add a \KNIMENODE{FeatureFinderMetabo} node (from \menu{Community Nodes > OpenMS > Quantitation} and connect the first output port of the \KNIMENODE{ZipLoopStart} to the \KNIMENODE{FeatureFinderMetabo}.
\item
For an optimal result adjust the following settings.
Please note that some of these are advanced parameters.

\begin{center}
\begin{tabular}{l|l}
\textbf{parameter} & \textbf{value} \\ \hline
\textit{algorithm $\rightarrow$ common $\rightarrow$ chrom\_fwhm} & $8.0$ \\
\textit{algorithm $\rightarrow$ mtd $\rightarrow$ trace\_termination\_criterion} & sample\_rate \\
\textit{algorithm $\rightarrow$ mtd $\rightarrow$ min\_trace\_length} & $3.0$ \\
\textit{algorithm $\rightarrow$ mtd $\rightarrow$ max\_trace\_length} & $600.0$ \\
\textit{algorithm $\rightarrow$ epd $\rightarrow$ width\_filtering} & off
\end{tabular}
\end{center}

\item
Add a \KNIMENODE{ZipLoopEnd} node and connect the output of the \KNIMENODE{FeatureFinderMetabo} to the first port of the \KNIMENODE{ZipLoopEnd} node.

\end{itemize}

To facilitate the collection of features corresponding to the same compound ion across different samples, an alignment of the samples' feature maps along retention time is often helpful. In addition to local, small-scale elution differences, one can often see constant retention time shifts across large sections between samples. We can use linear transformations to correct for these large scale retention differences. This  brings the majority of corresponding compound ions close to each other. Finding the correct corresponding ions is then faster and easier, as we don't have to search as far around individual features.

\begin{itemize}
\item
After the \KNIMENODE{ZipLoopEnd} node add a \KNIMENODE{MapAlignerPoseClustering} node (\menu{Community Nodes > OpenMS > Map Alignment}), set its Output Type to featureXML, and adjust the following settings

\begin{center}
\begin{tabular}{l|l}
\textbf{parameter} & \textbf{value} \\ \hline
\textit{algorithm $\rightarrow$ max\_num\_peaks\_considered} & $-1$ \\
\textit{algorithm $\rightarrow$ superimposer $\rightarrow$ mz\_pair\_max\_distance} & $0.005$ \\
\textit{algorithm $\rightarrow$ superimposer $\rightarrow$ num\_used\_points} & $10000$ \\
\textit{algorithm $\rightarrow$ pairfinder $\rightarrow$ distance\_RT $\rightarrow$ max\_difference} & $20.0$ \\
\textit{algorithm $\rightarrow$ pairfinder $\rightarrow$ distance\_MZ $\rightarrow$ max\_difference} & $20.0$ \\
\textit{algorithm $\rightarrow$ pairfinder $\rightarrow$ distance\_MZ $\rightarrow$ unit} & $ppm$
\end{tabular}
\end{center}

\end{itemize}

The next step after retention time correction is the grouping of corresponding features in multiple samples. In contrast to the previous alignment, we assume no linear relations of features across samples. The used method is tolerant against local swaps in elution order.


\begin{itemize}
\item
After the \KNIMENODE{MapAlignerPoseClustering} add a \KNIMENODE{FeatureLinkerUnlabeledQT} \\
 (\menu{Community Nodes > OpenMS > Map Alignment}) and adjust the following settings

\begin{center}
\begin{tabular}{l|l}
\textbf{parameter} & \textbf{value} \\ \hline
\textit{algorithm $\rightarrow$ distance\_RT $\rightarrow$ max\_difference} & $40.0$ \\
\textit{algorithm $\rightarrow$ distance\_MZ $\rightarrow$ max\_difference} & $20.0$ \\
\textit{algorithm $\rightarrow$ distance\_MZ $\rightarrow$ unit} & ppm
\end{tabular}
\end{center}

\item
After the \KNIMENODE{FeatureLinkerUnlabeledQT} add a \KNIMENODE{TextExporter} node (\menu{Community Nodes > OpenMS > File Handling}).
\item
Add an \KNIMENODE{Output Folder} node and configure it with an output directory where you want to store the resulting files.
\item
Run the pipeline and inspect the output.
\end{itemize}

You should find a single, tab-separated file containing the information on where metabolites were found and with which intensities.
You can also add \KNIMENODE{Output Folder} nodes at different stages of the workflow and inspect the intermediate results (e.g., identified metabolite features for each input map).
The complete workflow can be seen in \cref{fig:metabo_part1}.
In the following section we will try to identify those metabolites.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{graphics/metabo/metabo_part1}
  \caption{Label-free quantification workflow for metabolites}
  \label{fig:metabo_part1}
\end{figure}

\subsection{Identifying metabolites in LC-MS/MS samples}

At the current state we found several metabolites in the individual maps but so far don't know what they are.
To identify metabolites OpenMS provides multiple tools, including search by mass: the \KNIMENODE{AccurateMassSearch} node searches observed masses against the Human Metabolome Database (HMDB)\cite{Wishart2007,Wishart2009,Wishart2013}.
We start with the workflow from the previous section (see \cref{fig:metabo_part1}).

\begin{itemize}
\item
Add a \KNIMENODE{FileConverter} node and connect the output of the \KNIMENODE{FeatureLinkerUnlabeledQT} to the incoming port.
\item
Open the Configure dialog of the \KNIMENODE{FileConverter} and select the tab "OutputTypes".
In the drop down list for FileConverter.1.out select "featureXML".
\item
Add an \KNIMENODE{AccurateMassSearch} node and connect the output of the \KNIMENODE{FileConverter} to the first port of the \KNIMENODE{AccurateMassSearch}.
\item
Add four \KNIMENODE{Input File} nodes and configure them with the following files
\begin{itemize}
\item
\directory{Example\_Data / Metabolomics / databases / PositiveAdducts.tsv}\\
This file specifies the list of adducts that are considered in the positive mode. Each line contains the formula and charge of an adduct separated by a semicolon (e.g. M+H;1+). The mass of the adduct is calculated automatically.
\item
\directory{Example\_Data / Metabolomics / databases / NegativeAdducts.tsv}\\
This file specifies the list of adducts that are considered in the negative mode analogous to the positive mode.
\item
\directory{Example\_Data / Metabolomics / databases / HMDBMappingFile.tsv}\\
This file contains information from a metabolite database in this case from HMDB. It has three (or more) tab-separated columns: mass, formula, and identifier(s). This allows for an efficient search by mass.
\item
\directory{Example\_Data / Metabolomics / databases / HMDB2StructMapping.tsv}\\
This file contains additional information about the identifiers in the mapping file. It has four tab-separated columns that contain the identifier, name, SMILES, and INCHI. These will be included in the result file. The identifiers in this file must match the identifiers in the HMDBMappingFile.tsv.
\end{itemize}
\item
In the same order as they are given above connect them to the remaining input ports of the \KNIMENODE{AccurateMassSearch} node.
\item
Add an \KNIMENODE{Output Folder} node and connect the first output port of the \\
\KNIMENODE{AccurateMassSearch} node to the output folder.
\end{itemize}

The result of the \KNIMENODE{AccurateMassSearch} node is in the mzTab format \cite{Griss2014} so you can easily open it in a text editor or import it into Excel or KNIME, which we will do in the next section.
The complete workflow from this section is shown in \cref{fig:metabo_part2}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{graphics/metabo/metabo_part2}
  \caption{Label-free quantification and identification workflow for metabolites}
  \label{fig:metabo_part2}
\end{figure}


\subsection{Convert your data into a KNIME table}

The result from the \KNIMENODE{TextExporter} node as well as the result from the \KNIMENODE{AccurateMassSearch} node are files while standard KNIME nodes display and processes only KNIME tables. To convert these files into KNIME tables we need two different nodes.
For the \KNIMENODE{AccurateMassSearch} results we use the \KNIMENODE{MzTabReader} node (\menu{Community Nodes > OpenMS > Conversion > mzTab}), for the result of the \KNIMENODE{TextExporter} we use the \KNIMENODE{ConsensusTextReader} (\menu{Community Nodes > OpenMS > Conversion}).

When executed, both nodes will import the OpenMS files and provide access to the data as KNIME tables.
You can now easily combine both tables using the \KNIMENODE{Joiner} node (\menu{Data Manipulation > Column > Split \& Combine}) and configuring it to match the m/z and retention time values of the respective tables.
The full workflow is shown in \cref{fig:metabo_part3}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{graphics/metabo/metabo_part3_2015}
  \caption{Label-free quantification and identification workflow for metabolites that loads the results into KNIME and joins the tables.}
  \label{fig:metabo_part3}
\end{figure}

\subsubsection{Bonus task: Visualizing data}

Now that you have your data in KNIME you should try to get a feeling for the capabilities of KNIME.

\begin{task}
Check out the \KNIMENODE{Molecule Type Cast}  node (\menu{Chemistry > Translators}) together with subsequent cheminformatics nodes (e.g. \KNIMENODE{RDKit From Molecule} (\menu{Community Nodes > RDKit > Converters})) to render the structural formula contained in the result table.

\end{task}
\begin{task}
Have a look at the \KNIMENODE{Column Filter} node to reduce the table to the interesting columns, e.g., only the Ids, chemical formula, and intensities.
\end{task}
\begin{task}
Try to compute and visualize the m/z and retention time error of the different elements of the consensus features.
\end{task}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Downstream data analysis and reporting}
In this part of the metabolomics session we take a look at more advanced downstream analysis and the use of the statistical programming language R. As laid out in the introduction we try to detect a set of spike-in compounds against a complex blood background. As there are many ways to perform this type of analysis we provide a complete workflow.

\begin{task}
Import the workflow from \directory{Workflows / metabolite\_analysis.zip} in KNIME: \menu{File > Import KNIME Workflow...}
\end{task}

The section below will guide you in your understanding of the different parts of the workflow. Once you understood the workflow you should play around and be creative. Maybe create a novel visualization in KNIME or R? Do some more elaborate statistical analysis? Feel free to experiment and show us your results if you like. Note that some basic R knowledge is required to fully understand the processing in \KNIMENODE{R Snippet} nodes.

\subsubsection{Data preparation ID}
This part is analogous to what you did for the simple metabolomics pipeline.

\subsubsection{Data preparation Quant}
The first part is identical to what you did for the simple metabolomics pipeline. Additionally, we convert zero intensities into NA values and remove all rows that contain at least one NA value from the analysis. We do this using a very simple \KNIMENODE{R Snippet} and subsequent \KNIMENODE{Missing Value filter} node.

\begin{task}
Inspect the \KNIMENODE{R Snippet} by double-clicking on it. The KNIME table that is passed to an \KNIMENODE{R Snippet} node is available in R as a data.frame named knime.in. The result of this node will be read from the data.frame knime.out after the script finishes. Try to understand and evaluate parts of the script (Eval Selection). In this dialog you can also print intermediary results using for example the R command head() or cat() to the Console pane.
\end{task}

\subsubsection{Statistical analysis}
After we linked features across all maps, we want to identify features that are significantly deregulated between the two conditions. We will first scale and normalize the data, then perform a t-test, and finally correct the obtained p-values for multiple testing using Benjamini-Hochberg. All of these steps will be carried out in individual \KNIMENODE{R Snippet} nodes.
\begin{itemize}
\item Double-click on the first \KNIMENODE{R Snippet} node labeled "log scaling" to open the \KNIMENODE{R Snippet} dialog. In the middle you will see a short R script that performs the log scaling. To perform the log scaling we use a so-called regular expression (grepl) to select all columns containing the intensities in the six maps and take the $log_2$ logarithm.

\item The output of the log scaling node is also used to draw a boxplot that can be used to examine the structure of the data. Since we only want to plot the intensities in the different maps (and not m/z or rt) we first use a \KNIMENODE{Column Filter} node to keep only the columns that contain the intensities. We connect the resulting table to a \KNIMENODE{Box Plot} node which draws one box for every column in the input table. Right-click and select \menu{View: Box Plot}.

\item The median normalization is performed in a similar way to the log scaling. First we calculate the median intensity for each intensity column, then we subtract the median from every intensity.

\item Open the \KNIMENODE{Box Plot} connected to the normalization node and compare it to the box plot connected to the log scaling node to examine the effect of the median normalization.

\item To perform the t-test we defined the two groups we want to compare. Then we call the t-test for every consensus feature unless it has missing values. Finally we save the p-values and fold-changes in two new columns named p-value and FC.

\item The \KNIMENODE{Numeric Row Splitter} is used to filter less interesting parts of the data. In this case we only keep columns where the fold-change is $\geq$ 2.

\item We adjust the p-values for multiple testing using Benjamini-Hochberg and keep all consensus features with a q-value $\leq$ 0.01 (i.e. we target a false-discovery rate of $1\%$).

\end{itemize}


\subsubsection{Interactive visualization}

KNIME supports multiple nodes for interactive visualization with interrelated output. The nodes used in this part of the workflow exemplify this concept. They further demonstrate how 
figures with data dependent customization can be easily realized using basic KNIME nodes. Several simple operations are concatenated in order to enable an interactive volcano plot.
\begin{itemize}
\item We first log-transform fold changes and p-values in the \KNIMENODE{R Snippet} node. We then append columns noting interesting features (concerning fold change and p-value).
\item With this information, we can use various Manager nodes (\menu{Data Views > Property}) to emphasize interesting data points. The configuration dialogs allow us to select columns to change color, shape or size of data points dependent on the column values.
\item The \KNIMENODE{Scatter Plot} node (\menu{Data Views}) enables interactive visualization of the logarithmized values as a volcano plot: the log-transformed values can be chosen in the `Column Selection' tab of the plot view. Data points can be selected in the plot and HiLited via the menu option. HiLiteing transfers to all other interactive nodes connected to the same data table. In our case, selection and HiLiteing will also occur in the \KNIMENODE{Interactive Table} node (\menu{Data Views}).
\item Output of the interactive table can then be filtered via the HiLite menu tab. For example, we could restrict shown rows to points HiLited in the volcano plot.
\end{itemize}
\begin{task}
Inspect the nodes of this section. Customize your visualization and possibly try to visualize other aspects of your data.
\end{task}

\subsubsection{Advanced visualization}

R Dependencies: This section requires that the R packages ggplot2 and ggbiplot are both installed. ggplot2 is part of the \textit{KNIME R Statistics Integration (Windows Binaries)} installed during the tutorial setup section, ggbiplot however is not.
In case that you use an R installation where one or both of them are not yet installed, add an \KNIMENODE{R Snippet} node and double-click to configure. In the \textit{R Script} text editor, enter the following code:
\begin{lstlisting}
#Include the next line if you also have to install ggplot2:
install.packages("ggplot2")
#Include the following lines to install ggbiplot:
install.packages("devtools")
library(devtools)
install_github("vqv/ggbiplot")
\end{lstlisting}
Press \menu{Eval script} to execute the script.\newline

Even though the basic capabilities for (interactive) plots in KNIME are valuable for initial data exploration, professional looking depiction of analysis results often relies on dedicated plotting libraries. The statistics language R supports the addition of a large variety of packages, including packages providing extensive plotting capabilities. This part of the workflow shows how to use R nodes in KNIME to visualize more advanced figures. Specifically, we make use of different plotting packages to realize heatmaps and a PCA plot.


\begin{itemize}
\item The used \KNIMENODE{RView (Table)} nodes combine the possibility to write R snippet code with visualization capabilities inside KNIME. Resulting images can be looked at in the output RView, or saved via the \KNIMENODE{Image Port Writer} node.
\item The heatmap nodes make use of the gplots libary, which is by default part of the R Windows binaries for the KNIME 2.12 full installation. We again use regular expressions to extract all measured intensity columns for plotting. For clarity, feature names are only shown in the heatmap after filtering by fold changes.
\item The remaining node performs principal component analysis and plots the first two principal components. Data points are colored by group membership to treated or control samples. The R commands to install the necessary plotting package are part of the node snippet. Removal of the comment character `\#' and execution of the individual lines in the snippet node allows for package installation from inside KNIME.
\end{itemize}

\subsubsection{Data preparation for Reporting}
Following the identification, quantification and statistical analysis our data is merged and formatted for reporting.
First we want to discard our normalized and logarithmized intensity values in favor of the original ones.
To this end we first remove the intensity columns (\KNIMENODE{Column Filter}) and add the original intensities back (\KNIMENODE{Joiner}).
Note that we use an \textit{Inner Join} \footnote{\textit{Inner Join} is a technical term that describes how database tables are merged.}.
Combining ID and Quantification table into a single table is again achieved using a \KNIMENODE{Joiner} node.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{graphics/metabo/reporting_2015.png}
  \caption{Data preparation for reporting}
  \label{fig:consensusid}
\end{figure}

\begin{question}
What happens if we use an \textit{Left Outer Join}, \textit{Right Outer Join} or \textit{Full Outer Join} instead of the \textit{Inner Join}?
\end{question}

\begin{task}
Inspect the output of the join operation after the Molecule Type Cast and RDKit molecular structure generation.
\end{task}

While all relevant information is now contained in our table the presentation could be improved.
Currently, we have several rows corresponding to a single consensus feature (=linked feature) but with different, alternative identifications.
It would be more convenient to have only one row for each consensus feature with all accurate mass identifications added as additional columns.
To achieve we use the \KNIMENODE{Column to Grid} node that flattens several rows with the same consensus number into a single one.
Note that we have to specify the maximum number of columns in the grid so we set this to a large value (e.g. 100).
We finally export the data to an Excel file (\KNIMENODE{XLS Writer}).


